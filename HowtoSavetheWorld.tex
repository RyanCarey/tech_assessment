\documentclass[12pt]{article}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage[parfill]{parskip}
\usepackage{enumitem}
\usepackage{hyperref}


\title{Reducing Catastrophic Risks:\\ A Practical Introduction}
\author{Ryan Carey}

\begin{document}
\maketitle
So you're interested in playing a part in improving the future prospects for humanity? Welcome --- lots of us are, but 
where are we to start? The risks are many, and we have only so much time to learn about them. Hopefully, this document 
will help by presenting an overview of the thinking done on this question.

It's in five parts: 
\begin{enumerate}
    \item Which risks should we worry about?
    \item Approaches to mitigating 
    \item What can I personally do? 
    \item How does this differ from past advice? 
    \item Which organisations can I team up with?
\end{enumerate}

\section{Which risks should we worry about?}
Step one for reducing risks to humanity is thinking about which risks to focus on. 

The first key idea here, which is quite obvious once it's been said, is that the most important risks have to be those that are 
manmade because manmade risks operate on a shorter timescale. Life has existed on Earth for about 4.6 billion years and 
over that time it has withstood many natural catastrophes.
Humanity has been around for a few hundred thousand of those. But only for the last few thousand years have we performed 
tasks like writing, irrigation. Only a few centuries ago was the industrial revolution. 
Since that time, many new and transformative technologies have been developed, which we have only withstood for centuries, and 
in some cases for years or decades. So the risks that are most important for the present century all arise from our 
modern technologies.

This narrows the field considerably, but we're still left with lots of important possibilities. How big are the risks from climate change? From pandemics? Or from artificial intelligence (if that's something you worry about)? How should we prioritise these?

Which do we care about? Well, we care about the risks that are serious. But there's a threshold of seriousness that's especially important: we care most about the risks that are unrecoverable. If something comes along that sets technological development back by some decades, that's okay from a long-run view, because it doesn't set back humanity for its long-term future. If we're thinking on a geological timescale again, then we should remember that the Earth has a life expectancy in the hundreds of millions to billions of years. Given how the size to which our civilisation could grow, and the number of years we could live, our main job, here and now, is to simply to tide it through the next hundred.

What else can be said of our criterion for choosing risks to focus on? Well, we want risks that can be reduced, and where the efforts to mitigate them are uncrowded so that we can personally make a bigger difference.

For the purpose of this brief strategic overview, it's not necessary to go over all of the risks, but it will be handy to have examples of some of the top few. The Future of Humanity Institute are the top investigators of existential (i.e. unrecoverable) risk, and here is a fairly typical popular account of the risks rank the most concerning:
\begin{enumerate}
    \item Nuclear war
    \item Bioengineered pandemic
    \item Superintelligence
    \item Nanotechnology
    \item Unknown unknowns
\end{enumerate}

What's worth noting about these risks is that they're fairly diverse, and technical in nature. Although there are varying opinions about which of these risks is the largest, people don't usually place the top risk at more than 10x more probably than the next one. If the risks are technical, then it'll be necessary to have some people who are specific experts, but it'll also be handy to have people who are able to perform risk-management of more general varieties, as I'll discuss later.

What some people find surprising is that climate change did not make the list. It would probably make the top ten, 
but even if it did, it is more crowded - there is already an established community of researchers and advocates 
working on that problem, which cannot be said for these five. Another thing that bothers people is the mysterious 
point 5. If unknown unknowns are on our list - scenarios that can't even be predicted by our best experts - then 
what are we supposed to do? Perhaps we must find better experts or build some attributes of general resilience. 
Anyhow, the fact that the risks are sometimes obscure, in addition to being diverse, might shape our planning of 
how to respond to them.

So after that survey of the kinds of risks involved, it's time to get closer to the practical question of how we can fit the reduction of these risks into our lives. But we're not quite there yet. There's a bit more strategy - what are the kinds of activities that people would be doing that would make us safer from technological risks? This is the focus of the next section.

\section{Approaches to mitigating tech risk}
\subsection*{Research}
The big risks are technical, and many of them are still being discovered. So it's worth investing some resources into better forecasting emerging technologies, although this is a notoriously difficult task. It is also valuable to build safety features into these technologies, ideally before they are implemented, and to reach out to others who will do the same. Hence our first three approaches to tech risk mitigation:

\begin{enumerate}[label=\textbf{\arabic*})]
    \item {\bfseries Forecasting and macrostrategy.} e.g. The Future of Humanity Institute, The Centre for the Study of Existential Risk, The Open Philanthropy Project, Forecasting Science and Technology (ForeST), AI Impacts, technology assessment institutions, The Institute for Future Studies.
    \item {\bfseries Tech safety engineering.} e.g. The Machine Intelligence Research Institute.
\item {\bfseries Academic outreach.} Stuart Russell, The Future of Life Institute, The Global Catastrophic Risk Institute, Bulletin of Atomic Scientists, Edge event attendees, employees of Google Brain or DeepMind.
\end{enumerate}

\subsection*{Security}
A second critical component of the growing tech risk--mitigation infrastructure will be having professionals in the intelligence and cybersecurity communities who can liaise between researchers and government. There is also a role for more eclectic research into how to rebuild if something goes wrong.
\begin{enumerate}[label=\textbf{\arabic*})]
        \setcounter{enumi}{3}
    \item {\bfseries Cybersecurity \& International security (\(\mathbf{\pm}\) outreach).} Bruce Schneier, Palantir. David Denkenberger, safety barriers, surveillance generally (perhaps)
\end{enumerate}

\subsection*{Policy making}
Another critical link in the chain from research to implementation will be the policy makers. Currently, the number of people who have or are pursuing a theoretical understanding of risky technologies is more than the number who are willing or able to implement these in government. Currently, we don't know what policies to promote to reduce catastrophic risks, so there will be a slow transition through policy development and eventually to implementation.
\begin{enumerate}[label=\textbf{\arabic*})]
        \setcounter{enumi}{4}
    \item {\bfseries Tech policy development (\(\pm\) outreach).} The Centre for the Study of Existential Risk, some The Future of Life Institute grantees, The Center for International Security and Cooperation, The Royal United Services Institute, The British American Security Information Council.
\item {\bfseries Politics.} Ministers of defence, foreign affairs and science.
        \item {\bfseries Public service.} In intelligence, cybersecurity, intelligence research and defence.
\end{enumerate}

\subsection*{Broad-based outreach}
For some individuals, the best way to reach individuals who can help with the above problems is going to be to do broad-based community outreach. The presence of such organisations gives a plausible story for how the above activities were supported and coordinated.
\begin{enumerate}[label=\textbf{\arabic*})]
        \setcounter{enumi}{7}
\item {\bfseries Future-building outreach.} The Centre for Effective Altruism, The Future of Life Institute, The Center for Applied Rationality, The Machine Intelligence Research Institute, The Centre for the Study of Existential Risk, Stephen Hawking.
\end{enumerate}

\subsection*{Funding}
Lastly, many of the above projects, especially the broad-based outreach, but also some of the forecasting and safety engineering efforts will require philanthropic contribution.
\begin{enumerate}[label=\textbf{\arabic*})]
        \setcounter{enumi}{8}
    \item {\bfseries Funding.} Elon Musk, Jaan Tallin, Skoll Threats Fund, Good Ventures, tech-related foundation program managers, partners for venture capitalists, effective altruists.
\end{enumerate}

This list is not exhaustive. Other activities could be included, for examples a special category could be made for technological journalism or for strategic investment in risk-relevant tech companies. But most of the major bases are covered.

\section{What can I personally do?}
Finally, let's talk about concrete personal plans, with an emphasis on our careers, which is where we spend the majority of our efforts.

For someone who cares about tech risks and is already in a relevant field, such as a governmental minister, probably the best thing for them to do is to stay at it, or just pivot into the nearest of these risk mitigation modes. For researchers in relevant fields that are not specific to any one tech (think economics, or some parts of philosophy), a useful approach will be to try and collaborate with risk-focussed organisations.

For people underway in less-relevant fields, there are often still opportunities to cross-over to discussing tech. For example, Dylan Matthews is making a career of discussing important issues as a journalist. Careers like marketing, management and executive assisting work can fit into basically any of these pathways, depending on the job opportunities that arise for a given individual. Clearly, not even in a research organisation should every single individual be a complicated genius. Plenty of more rounded individuals are needed everywhere to make ambitious projects run properly and interact smoothly with other organisations.

For people who are just starting out, it's going to be an issue of finding people's comparative advantages. Policy-minded people should seriously advancing that interest to fill out the movement with a skillset in which it is currently lacking. Obviously skills in networking and coalition-building are required. People with a talent for theoretical computer science and discrete mathematics should see whether they can usefully contribute to The Machine Intelligence Research Institute. People with great academic and technical ability may suit academic research, and so on. Funding is another task, like management and executive assisting, 
that nearly anyone can contribute to. Though probably less important than aptitude, personal interest in a subject is also a factor.

There's a lot more discussion to be had on the topic of career selection, but much of it is person-dependent, so the best I can hope is that this framework will help to ground that ongoing discussion.

\section{How does this differ from past advice?}
There's been articles about risk reduction before, but some are focussed on specific risks or explain detailed results of investigations. Here, I've tried to give a readable strategic overview.

So how does this strategic overview differ from what someone else might write, or what people might've written several years ago?

For good reasons, over the past decade, risk researchers have focussed on building a solid theoretical basis for risk 
assessment. Now that our perceptions of the top risks are stabilising, the emphasis is moving toward outreaching to academics and 
policy makers who can advise and drive the implementation of these policies. In that regard, the list is different from what it might've been a few years ago.

Another way my thinking differs from what's come previously is that I try to take replaceability seriously: being involved in developing risky technologies like AI or surveillance might not be a terribly bad thing, if it is going to be done anyway, because then you get to sound alarm bells if risk becomes elevated, while networking with people in an important space. The risk associated with speeding up a technology's development might be outweighed by the networking benefits from participating in that field.

On a related note, discussions are taking a more conciliatory tone to technology developers than previously. This is because 
there are more discussions with both technologists and policy makers than previously. In the first party, there is some growing concern about scaremongering with Terminator pictures and the like on journal articles, which is received as somewhat antagonistic. In policy makers, there is a desire for unified answers from technologists, security experts and risk researchers. So building bridges between the above is critical for moving the discourse to the next level, and if for this reason and no other, all of these are plausible areas of work to aim to get into.

Since the outreach is supposed to be more targeted to people in tech and security, and oriented toward action, I've put `future building' as a placeholder for the kind of movement that one would want to develop, though the rationality and effective altruism communities are the main ones that have so far been working in and around this space.

Apart from these points, I think I've just summarised what a lot of people have for many years been thinking.
\section{Which organisations can I team up with?}
It's very useful to learn about organisations working on addressing these problems, to assess what one might personally be able to contribute.

Of the organisations mentioned above, a handful deserve special emphasis:
\begin{itemize}
    \item The Machine Intelligence Research Institute (MIRI): performs technical AI safety research using discrete math
    \item The Centre for the Study of Existential Risk (CSER): applies various sciences to risk forecasting and reduction
    \item The Future of Humanity Institute (FHI): leads in macrostrategy research, founded by Nick Bostrom.
    \item The Future of Life Institute (FLI): fundraises and does academic outreach. Distributed funds from Elon Musk and hosted a successful AI safety conference in Puerto Rico.
    \item The Global Catastrophic Risk Institute (GCRI): research and academic outreach in relation to resilience and other topics.
    \item The Open Philanthropy Project (Open Phil): an offshoot of GiveWell that allocates funds on behalf of its partner foundation Good Ventures for catastrophic risk reduction and other causes.
\end{itemize}

You can read more about all of these topics at \href{www.existential-risk.org}{www.existential-risk.org}, a website by FHI.

Over the next decade, hopefully many more such organisations will be founded.

\section{Conclusion}
The kinds of people we need to reduce tech risks are different from what we needed a decade ago. We need to act in at least nine domains: 1. Forecasting and macrostrategy. 2. Tech safety engineering. 3. Academic outreach. 4. Cybersecurity + International security 5. Tech policy development. 6. Politics. 7. Public service. 8. Future-building outreach. and 9. Funding.

\vspace{3em}

{\itshape Thanks to Owen Cotton-Barratt, Niel Bowerman and Haydn Belfield for feedback on an earlier draft.}
\end{document}
